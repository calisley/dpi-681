{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e867ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda6eb5",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "API Key and System Prompt Settings\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6d9c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly set your OpenAI API key here:\n",
    "OPENAI_API_KEY = \"\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the system prompt (context for the AI assistant) here:\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant specialized in generative AI and public policy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fe823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key from the variable or fallback to environment variable.\n",
    "openai.api_key = OPENAI_API_KEY if OPENAI_API_KEY else os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not openai.api_key:\n",
    "    raise ValueError(\"Missing API key: Please set your OpenAI API key in the code or as an environment variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b566f2aa",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "Configuration Variables\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22a20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model to be used (set to the newest client model, e.g., \"gpt-4\")\n",
    "MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag: set to True to stream the chain-of-thought response.\n",
    "STREAM_CHAIN_OF_THOUGHT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5610ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the client\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36158477",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Global conversation history: stores a list of messages (each a dict with 'role' and 'content')\n",
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de50536",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "Functions\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9052c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_query(user_input, system_prompt=SYSTEM_PROMPT, model=MODEL, stream=STREAM_CHAIN_OF_THOUGHT):\n",
    "    \"\"\"\n",
    "    Sends a query to the OpenAI chat model with the previous context and returns the response.\n",
    "\n",
    "    Parameters:\n",
    "        user_input (str): The user's input query.\n",
    "        system_prompt (str): The system prompt to establish context.\n",
    "        model (str): The OpenAI model to use.\n",
    "        stream (bool): If True, stream the chain-of-thought response.\n",
    "\n",
    "    Returns:\n",
    "        str: The full response from the AI.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    # Build the messages list: include the system message and last 10 conversation messages.\n",
    "    # Note: conversation_history already contains both user and assistant messages.\n",
    "    context = conversation_history[-10:]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + context\n",
    "\n",
    "    # If the latest user query isn't in the conversation history yet, add it.\n",
    "    if not context or context[-1].get(\"role\") != \"user\" or context[-1].get(\"content\") != user_input:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            stream=stream\n",
    "        )\n",
    "    except openai.RateLimitError:\n",
    "        print(\"Error: Rate limit exceeded. Please wait and try again later.\")\n",
    "        return \"\"\n",
    "    except (openai.APIError, openai.APIConnectionError) as e:\n",
    "        print(f\"Error: API error encountered - {e}\")\n",
    "        return \"\"\n",
    "    except openai.BadRequestError as e:\n",
    "        print(f\"Error: Invalid request - {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    full_response = \"\"\n",
    "    \n",
    "    if stream:\n",
    "        print(\"\\nOpenAI API: \", end=\"\", flush=True)\n",
    "        try:\n",
    "            # Iterate over the streamed chunks and print tokens as they arrive.\n",
    "            for chunk in response:\n",
    "                if chunk.choices:\n",
    "                    delta = chunk.choices[0].delta\n",
    "                    # Check if delta.content exists before printing\n",
    "                    if delta.content:\n",
    "                        content = delta.content\n",
    "                        print(content, end=\"\", flush=True)\n",
    "                        full_response += content\n",
    "            print()  # New line after streaming.\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during streaming response: {e}\")\n",
    "    else:\n",
    "        try:\n",
    "            full_response = response.choices[0].message.content\n",
    "            print(\"\\nOpen API:\", full_response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing response: {e}\")\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842662f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the command-line interface.\n",
    "    \"\"\"\n",
    "    global conversation_history\n",
    "    print(\"Welcome to the ChatGPT-CLI!\")\n",
    "    print(\"Type 'exit' or 'quit' to end the session.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter your query: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Append the user's message to the conversation history.\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Process the query and get the AI's response.\n",
    "        assistant_response = make_query(user_input)\n",
    "\n",
    "        # Append the assistant's response to the conversation history.\n",
    "        if assistant_response:\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955365b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9f81c",
   "metadata": {},
   "source": [
    "# Part 1: Your First API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8830ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5131f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write a haiku about bananas.\"\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b87aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
