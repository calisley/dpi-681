{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d140804",
   "metadata": {},
   "source": [
    "\n",
    "# Section 1 · ChatGPT Clone (Colab Edition)\n",
    "\n",
    "This notebook gives you a minimal, working ChatGPT-like loop plus a set of **policy-focused prompts** for your Vibe‑Coding Challenge.  \n",
    "Run cells **top to bottom**.\n",
    "\n",
    "**In today's section, you'll**\n",
    "1. Add your API key (private to this runtime).  \n",
    "2. Send your **first request** to the model.  \n",
    "3. Use a small chat loop with **streaming tokens**.  \n",
    "4. Try policy-oriented templates: haiku/poem, summary bot, interactive issue chat, and a debate simulator.\n",
    "\n",
    "> Privacy note: Your API key stays in this runtime. Avoid sharing the notebook after entering your key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b31f9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports (just run)\n",
    "No extra installs needed in Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1523e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7a17d",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Add your API key and set a system prompt\n",
    "- If you already set `OPENAI_API_KEY` in the environment, this will pick it up.\n",
    "- Otherwise you'll be prompted *once* (input is hidden).  \n",
    "- You can customize the system prompt at any time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# API Key and System Prompt\n",
    "# ===============================\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    try:\n",
    "        import getpass\n",
    "        OPENAI_API_KEY = getpass.getpass(\"Enter your OpenAI API key (input hidden): \").strip()\n",
    "    except Exception:\n",
    "        raise ValueError(\"Unable to capture API key input. Set os.environ['OPENAI_API_KEY'] manually.\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Missing API key. Please provide a valid OpenAI API key.\")\n",
    "\n",
    "# Make the key available to the SDK\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# You can tweak this later (see Section 6)\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant specialized in generative AI and public policy.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd5758",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Configuration & global state\n",
    "- Pick a model (you can change later).  \n",
    "- Turn token streaming on/off.  \n",
    "- A simple `conversation_history` stores the last messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec967931",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# Configuration\n",
    "# ===============================\n",
    "\n",
    "MODEL = \"gpt-5-mini\"   # You can swap to \"gpt-4o\" or \"gpt-4o-mini\" if available\n",
    "STREAM_TOKENS = True   # stream tokens to the output\n",
    "\n",
    "# Instantiate a single client for the session\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Global conversation history (list of dicts with 'role' and 'content')\n",
    "conversation_history: List[Dict[str, str]] = []\n",
    "\n",
    "def reset_history() -> None:\n",
    "    \"\"\"Clear the global conversation history.\"\"\"\n",
    "    conversation_history.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2555d",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Core chat function (with streaming + simple retries)\n",
    "**What this cell does**\n",
    "- Builds a message list from the system prompt + recent history + your new user input.  \n",
    "- Sends a request to the Chat Completions API.  \n",
    "- If streaming is on, it prints tokens as they arrive.  \n",
    "- Appends both user/assistant turns to `conversation_history`.\n",
    "\n",
    "If you get a rate limit, it retries briefly with exponential backoff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_query(\n",
    "    user_input: str,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    model: str = MODEL,\n",
    "    stream: bool = STREAM_TOKENS,\n",
    "    max_history: int = 10,\n",
    "    retries: int = 2,\n",
    "    backoff_seconds: float = 2.0,\n",
    ") -> str:\n",
    "    \"\"\"Send a query with context and return the assistant's text.\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    # Build messages (system + recent context + current user)\n",
    "    context = conversation_history[-max_history:]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + context\n",
    "\n",
    "    if not context or context[-1].get(\"role\") != \"user\" or context[-1].get(\"content\") != user_input:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                stream=stream,\n",
    "            )\n",
    "\n",
    "            full_text = \"\"\n",
    "\n",
    "            if stream:\n",
    "                print(\"\\nAssistant:\", end=\" \", flush=True)\n",
    "                for chunk in resp:\n",
    "                    if chunk.choices:\n",
    "                        delta = chunk.choices[0].delta\n",
    "                        if delta and getattr(delta, \"content\", None):\n",
    "                            token = delta.content\n",
    "                            print(token, end=\"\", flush=True)\n",
    "                            full_text += token\n",
    "                print()  # newline after stream\n",
    "            else:\n",
    "                full_text = resp.choices[0].message.content\n",
    "                print(\"\\nAssistant:\", full_text)\n",
    "\n",
    "            # Update history\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": full_text})\n",
    "            return full_text\n",
    "\n",
    "        except openai.RateLimitError as e:\n",
    "            if attempt >= retries:\n",
    "                print(f\"\\nRate limit error after {attempt+1} attempt(s): {e}\")\n",
    "                return \"\"\n",
    "            delay = backoff_seconds * (2 ** attempt)\n",
    "            print(f\"\\nRate limit. Retrying in {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "            attempt += 1\n",
    "        except (openai.APIError, openai.APIConnectionError, openai.BadRequestError) as e:\n",
    "            print(f\"\\nAPI error: {e}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"\\nUnexpected error: {e}\")\n",
    "            return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640cc2b",
   "metadata": {},
   "source": [
    "\n",
    "## 5) One-shot helper\n",
    "Use `chat_once(\"your prompt\")` for quick tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a66290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_once(prompt: str, **kwargs) -> str:\n",
    "    \"\"\"Convenience wrapper to send a single prompt and return text.\"\"\"\n",
    "    return make_query(prompt, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee680e0",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Your first API request\n",
    "Just run this cell. If streaming is on, tokens appear as they arrive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = chat_once(\"Write a haiku about bananas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75831f",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) Tweak the system prompt\n",
    "- Change the tone or role of the assistant.  \n",
    "- We reset the history so the new system prompt takes effect cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = \"You are a concise assistant for generative AI and public policy. Keep answers tight and specific.\"\n",
    "reset_history()\n",
    "\n",
    "_ = chat_once(\"In one paragraph, contrast broad LLMs with narrow task-specific models for policy analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f63ceb",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2 · Vibe‑Coding Challenge (Policy)\n",
    "\n",
    "**Goal:** Use generative AI as your *primary coder/author* to rapidly prototype ideas around a policy topic.\n",
    "\n",
    "**What to try**\n",
    "- Start simple (haiku/poem) and then move to summaries, debates, or an interactive issue chat.\n",
    "- Modify prompts. Combine outputs. Iterate quickly.\n",
    "- There’s no single correct approach; we’ll share takeaways at the end.\n",
    "\n",
    "**Timebox:** Work until ~5–10 minutes before the end; be ready to share.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0490c",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Policy‑themed haiku or poem (easy)\n",
    "Change the topic to something you care about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bddc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = chat_once(\"Write a haiku about the challenges of urban housing policy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2c43b",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Policy summary bot (medium)\n",
    "Paste a short excerpt (or a brief description) below. If the text is long, summarize the relevant parts first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "policy_text = \"\"\"\n",
    "[Paste or briefly describe a policy, report, or issue here. If long, summarize key excerpts.]\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are a policy analysis bot.\n",
    "1) Read the policy content below.\n",
    "2) Provide a concise summary (bulleted).\n",
    "3) Suggest a creative, actionable recommendation (2-3 bullets).\n",
    "4) Flag key tradeoffs and uncertainties.\n",
    "\n",
    "POLICY CONTENT:\n",
    "{policy_text}\n",
    "\"\"\"\n",
    "\n",
    "_ = chat_once(prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567710ff",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Interactive “issue of interest” chat (hard)\n",
    "Enter any policy issue; re-run as many times as you want.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf13746",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "issue = input(\"Enter a policy issue of interest: \").strip()\n",
    "_ = chat_once(f\"Discuss the policy implications of {issue} in a creative and engaging manner.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12c01f",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Debate simulator (easy → hard)\n",
    "Start small with a pro/con. Then try two distinct 'agents' and a neutral moderator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple version\n",
    "_ = chat_once(\"Provide one argument in favor of and one argument against universal basic income.\")\n",
    "\n",
    "# Two-agent extension (sketch)\n",
    "agent_a = \"You are a policy analyst focused on efficiency and growth.\"\n",
    "agent_b = \"You are a policy analyst focused on equity and distribution.\"\n",
    "\n",
    "reset_history()\n",
    "_ = chat_once(f\"As {agent_a} Give a short opening statement on universal basic income.\")\n",
    "_ = chat_once(f\"As {agent_b} Respond with a short opening statement on universal basic income.\")\n",
    "_ = chat_once(\"As a neutral moderator, summarize the strongest point from each side and note the main point of disagreement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab490961",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Optional: Minimal chat loop inside Colab\n",
    "Type messages; type `exit` or `quit` (or interrupt the cell) to stop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reset_history()\n",
    "print(\"Chat started. Type 'exit' to stop.\\n\")\n",
    "while True:\n",
    "    try:\n",
    "        user_in = input(\"You: \").strip()\n",
    "    except EOFError:\n",
    "        break\n",
    "    if user_in.lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    _ = make_query(user_in)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef4b3c",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Optional: Responses API (one-shot)\n",
    "For this section, the Chat Completions examples above are sufficient.  \n",
    "If you want to experiment, here's a quick one-shot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI as _OpenAI\n",
    "_client = _OpenAI()\n",
    "\n",
    "resp = _client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=\"Write a 2-sentence policy memo about congestion pricing.\"\n",
    ")\n",
    "print(resp.output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
